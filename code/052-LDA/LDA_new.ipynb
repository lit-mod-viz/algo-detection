{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import itertools\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim import similarities\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords \n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting the count of each word in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "def count_words(word_list):\n",
    "    for w in word_list:\n",
    "        if w in word_dict.keys():\n",
    "            word_dict[w] = word_dict[w] + 1\n",
    "        else:\n",
    "            word_dict[w] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading each document as token of words(lemmatized) in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "doc_corpus = []\n",
    "def document_tokens(source):\n",
    "    files = glob.glob(source+'/*.txt') \n",
    "    combined_txt = ''\n",
    "    for file in files:\n",
    "        in_file = open(file,'r')\n",
    "        txt = in_file.read()\n",
    "        lemmatized_tokens = []\n",
    "        tokens = word_tokenize(txt)\n",
    "        lemmatized_tokens = [w for w in tokens if len(w)!=1] \n",
    "        count_words(lemmatized_tokens) \n",
    "        doc_corpus.append(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to read: 24.906488080819447\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "document_tokens(\"../pre-processing/clean-source\")\n",
    "document_tokens(\"../pre-processing/clean-target\")\n",
    "document_tokens(\"../pre-processing/clean-test/clean-suspected-algo\")\n",
    "document_tokens(\"../pre-processing/clean-test/clean-suspected-no-algo\")\n",
    "document_tokens(\"../pre-processing/clean-fullcorpus\")\n",
    "print(\"Time taken to read:\", (time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total documents: 11612\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of total documents:\",len(doc_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words = [(k, word_dict[k]) for k in sorted(word_dict, key=word_dict.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique words in the whole corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5330376"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing top 50 words and the words whose frequency is 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_remove = []\n",
    "for s in sorted_words[:50]:\n",
    "    words_remove.append(s[0])\n",
    "    \n",
    "for s in sorted_words[::-1]:\n",
    "    if s[1]==1:\n",
    "        words_remove.append(s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-914b91227301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mshort_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshort_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_remove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time taken to remove words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-914b91227301>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mshort_corpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mshort_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords_remove\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Time taken to remove words:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "short_corpus=[]\n",
    "for d in range(0,len(doc_corpus)):\n",
    "    short_corpus.append([i for i in doc_corpus[d] if i not in words_remove])\n",
    "    \n",
    "print(\"Time taken to remove words:\", (time.time()-start_time)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique words reduced to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = [word for item in short_corpus for word in item]\n",
    "# use nltk fdist to get a frequency distribution of all words\n",
    "fdist = FreqDist(all_words)\n",
    "len(fdist) # number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_doc = []\n",
    "len_short=[]\n",
    "for i in range(0,len(doc_corpus)):\n",
    "    len_doc.append(len(doc_corpus[i]))\n",
    "    len_short.append(len(short_corpus[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 9)\n",
    "fig, axes = plt.subplots(1, 2, figsize=a4_dims)\n",
    "axes[0].plot(len_doc)\n",
    "axes[0].set_title(\"Number of words in every document before preprocessing\")\n",
    "axes[1].plot(len_short)\n",
    "axes[1].set_title(\"Number of words in every document after preprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(short_corpus)\n",
    "\n",
    "# Create Corpus\n",
    "texts = short_corpus\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modelling for topics=5 to 15. Best results at 8. (Without using Multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "coherence_score = []\n",
    "perplexity = []\n",
    "topics = [i for i in range(5,15)]\n",
    "for j in range(0,len(topics)):\n",
    "    t1 = time.time()\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=topics[j],random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)\n",
    "    #lda_model.save(\"models/lda_model\" + str(j) +\".model\")\n",
    "    perplexity.append(lda_model.log_perplexity(corpus))\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=short_corpus, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score.append(coherence_model_lda.get_coherence())\n",
    "    t2 = time.time()\n",
    "    print(\"Time taken to model for\",topics[j],'topics=',(t2-t1)/60,\"mins.\")\n",
    "    \n",
    "print(\"Total time taken:\",(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 6)\n",
    "fig, axes = plt.subplots(1, 2, figsize=a4_dims)\n",
    "\n",
    "axes[0].plot(topics,coherence_score ,marker='o', markerfacecolor='blue', markersize=12, color='#E69F00', linewidth=4)\n",
    "axes[0].set_title('Distribution of Coherence Score by number of topics')\n",
    "axes[0].set_xlabel('Number of topics')\n",
    "axes[0].set_ylabel('Coherence score')\n",
    "\n",
    "axes[1].plot(topics,perplexity ,marker='o', markerfacecolor='blue', markersize=12, color='#E69F00', linewidth=4)\n",
    "axes[1].set_title('Distribution of Perplexity by number of topics')\n",
    "axes[1].set_xlabel('Number of topics')\n",
    "axes[1].set_ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modelling for topics=5 to 15. (With using Multicore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "coherence_score = []\n",
    "perplexity = []\n",
    "topics = [i for i in range(5,15)]\n",
    "for j in range(0,len(topics)):\n",
    "    t1 = time.time()\n",
    "    lda_model = LdaMulticore(corpus=corpus,id2word=id2word,num_topics=topics[j],random_state=100,chunksize=100, \n",
    "                             passes=10, workers=3)\n",
    "    #lda_model.save(\"models/lda_model\" + str(j) +\".model\")\n",
    "    perplexity.append(lda_model.log_perplexity(corpus))\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=short_corpus, dictionary=id2word, coherence='c_v')\n",
    "    coherence_score.append(coherence_model_lda.get_coherence())\n",
    "    t2 = time.time()\n",
    "    print(\"Time taken to model for\",topics[j],'topics=',(t2-t1)/60,\"mins.\")\n",
    "    \n",
    "print(\"Total time taken:\",(time.time()-t)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4_dims = (15, 6)\n",
    "fig, axes = plt.subplots(1, 2, figsize=a4_dims)\n",
    "\n",
    "axes[0].plot(topics,coherence_score ,marker='o', markerfacecolor='blue', markersize=12, color='#E69F00', linewidth=4)\n",
    "axes[0].set_title('Distribution of Coherence Score by number of topics')\n",
    "axes[0].set_xlabel('Number of topics')\n",
    "axes[0].set_ylabel('Coherence score')\n",
    "\n",
    "axes[1].plot(topics,perplexity ,marker='o', markerfacecolor='blue', markersize=12, color='#E69F00', linewidth=4)\n",
    "axes[1].set_title('Distribution of Perplexity by number of topics')\n",
    "axes[1].set_xlabel('Number of topics')\n",
    "axes[1].set_ylabel('Perplexity')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modelling with best results, i.e. topics=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,id2word=id2word,num_topics=8,random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of words shown for 5 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = lda_model.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
